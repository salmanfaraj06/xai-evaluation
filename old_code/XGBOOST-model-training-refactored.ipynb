{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984dae91",
   "metadata": {},
   "source": [
    "\n",
    "# XGBoost Credit Default (Leak-Free CV + Calibration)\n",
    "\n",
    "This notebook refactors the earlier experiment to provide:\n",
    "- Leak-free evaluation: CV only on the train fold; the test set stays fully held out.\n",
    "- Variance estimates: mean ± CI for ROC-AUC, PR-AUC, Brier, log loss across CV and bootstrap on test.\n",
    "- Expanded but tractable search with regularization knobs.\n",
    "- Probability calibration and cost-aware thresholding plus top-k capture/lift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf99a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Environment ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If a dependency is missing, install it. Skip network install if already present.\n",
    "import importlib.util, subprocess, sys\n",
    "\n",
    "def ensure(pkg):\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "for pkg in ['pandas', 'numpy', 'xgboost', 'scikit-learn']:\n",
    "    ensure(pkg)\n",
    "\n",
    "print('Environment ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f3b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63130947",
   "metadata": {},
   "source": [
    "\n",
    "## Load and inspect data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edb8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (255347, 18)\n",
      "Columns: ['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default']\n",
      "Missing values per column:\n",
      "LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Target distribution (raw):\n",
      "Default\n",
      "0    0.883872\n",
      "1    0.116128\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = Path('loan_default.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', df.columns.tolist())\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print('Target distribution (raw):')\n",
    "print(df['Default'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99a78b",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocess\n",
    "- Drop ID-like columns.\n",
    "- Harmonize binary flags (Yes/No -> 1/0).\n",
    "- One-hot encode categoricals.\n",
    "- Keep numerics as-is; fill residual NAs defensively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858bd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix shape: (255347, 24)\n",
      "Target distribution: Counter({0: 225694, 1: 29653})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop ID-style columns if present\n",
    "id_cols = ['LoanID', 'loan_id', 'ID', 'id']\n",
    "df = df.drop(columns=[c for c in id_cols if c in df.columns])\n",
    "\n",
    "# Normalize binary yes/no style flags\n",
    "binary_cols = ['HasMortgage', 'HasDependents', 'HasCoSigner']\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == bool:\n",
    "            df[col] = df[col].astype(int)\n",
    "        else:\n",
    "            df[col] = df[col].astype(str).str.strip().str.upper().map({'YES': 1, 'NO': 0})\n",
    "\n",
    "# Identify types\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "num_cols = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Fill any remaining NAs defensively\n",
    "if len(num_cols) > 0:\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "if len(cat_cols) > 0:\n",
    "    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n",
    "\n",
    "categorical_features = ['Education', 'EmploymentType', 'MaritalStatus', 'LoanPurpose']\n",
    "categorical_features = [c for c in categorical_features if c in df.columns]\n",
    "\n",
    "# One-hot encode selected categoricals\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "TARGET_COL = 'Default'\n",
    "if TARGET_COL not in df_encoded.columns:\n",
    "    raise ValueError(f\"Target column {TARGET_COL} not found.\")\n",
    "\n",
    "X = df_encoded.drop(TARGET_COL, axis=1)\n",
    "y = df_encoded[TARGET_COL]\n",
    "\n",
    "print('Final feature matrix shape:', X.shape)\n",
    "print('Target distribution:', Counter(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53a663",
   "metadata": {},
   "source": [
    "\n",
    "## Split: Train/Validation pool vs Test (held out)\n",
    "Test remains untouched until the very end. All CV happens on the train/validation pool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a29d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Val shape: (204277, 24)\n",
      "Test shape     : (51070, 24)\n",
      "Train+Val target dist: Counter({0: 180555, 1: 23722})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print('Train+Val shape:', X_trainval.shape)\n",
    "print('Test shape     :', X_test.shape)\n",
    "print('Train+Val target dist:', Counter(y_trainval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de406522",
   "metadata": {},
   "source": [
    "\n",
    "## Helper functions: metrics, CI, thresholds, top-k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db36ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def metric_bundle(y_true, proba) -> Dict[str, float]:\n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(y_true, proba),\n",
    "        'pr_auc': average_precision_score(y_true, proba),\n",
    "        'brier': brier_score_loss(y_true, proba),\n",
    "        'log_loss': log_loss(y_true, proba),\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_ci(metric_fn, y_true, proba, n_iter: int = 500, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    scores = []\n",
    "    y_arr, p_arr = np.array(y_true), np.array(proba)\n",
    "    for _ in range(n_iter):\n",
    "        idx = rng.integers(0, len(y_arr), len(y_arr))\n",
    "        scores.append(metric_fn(y_arr[idx], p_arr[idx]))\n",
    "    return np.percentile(scores, [2.5, 50, 97.5])\n",
    "\n",
    "\n",
    "def threshold_table(y_true, proba, thresholds=None, cost_fn: float = 10.0, cost_fp: float = 1.0):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    base_rate = y_true.mean()\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        y_pred = (proba >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
    "        bal_acc = 0.5 * (spec + rec)\n",
    "        cost = cost_fn * fn + cost_fp * fp\n",
    "        lift = (prec / base_rate) if base_rate > 0 else np.nan\n",
    "        rows.append({\n",
    "            'threshold': thr,\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1,\n",
    "            'balanced_accuracy': bal_acc,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'tp': tp,\n",
    "            'cost': cost,\n",
    "            'lift': lift,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def pick_scenarios(thr_df: pd.DataFrame, cost_fn: float, cost_fp: float):\n",
    "    rows = []\n",
    "    idx_acc = thr_df['accuracy'].idxmax()\n",
    "    idx_f1 = thr_df['f1'].idxmax()\n",
    "    idx_bal = thr_df['balanced_accuracy'].idxmax()\n",
    "    idx_cost = thr_df['cost'].idxmin()\n",
    "    candidates = {'max_accuracy': idx_acc, 'max_f1': idx_f1, 'max_balanced_accuracy': idx_bal, f'min_cost_FN{int(cost_fn)}_FP{int(cost_fp)}': idx_cost}\n",
    "    high_rec = thr_df[thr_df['recall'] >= 0.70]\n",
    "    if not high_rec.empty:\n",
    "        candidates['recall>=0.70_max_accuracy'] = high_rec['accuracy'].idxmax()\n",
    "    for name, idx in candidates.items():\n",
    "        row = thr_df.loc[idx].to_dict()\n",
    "        row['scenario'] = name\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def topk_capture(y_true, proba, fracs: List[float]):\n",
    "    base_rate = y_true.mean()\n",
    "    order = np.argsort(-proba)\n",
    "    rows = []\n",
    "    for frac in fracs:\n",
    "        k = max(1, int(len(proba) * frac))\n",
    "        mask = np.zeros_like(proba, dtype=int)\n",
    "        mask[order[:k]] = 1\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, mask).ravel()\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        lift = (prec / base_rate) if base_rate > 0 else np.nan\n",
    "        rows.append({\n",
    "            'scenario': f'top_{int(frac*100)}pct',\n",
    "            'volume_pct': frac,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'lift': lift,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'tp': tp,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333e741",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter space (compact but richer than before)\n",
    "- Adds regularization (`gamma`, `reg_lambda`, `reg_alpha`) and `max_delta_step`.\n",
    "- Uses `tree_method='hist'` for speed on 250k rows.\n",
    "- Early stopping on each fold to control overfitting; we capture the median best iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7d2e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = [\n",
    "    {'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'eta': 0.05},\n",
    "    {'max_depth': 3, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 0.25, 'reg_lambda': 3.0, 'reg_alpha': 0.0, 'eta': 0.05},\n",
    "    {'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 1.0, 'gamma': 0.0, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'eta': 0.05},\n",
    "    {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.85, 'colsample_bytree': 1.0, 'gamma': 0.5, 'reg_lambda': 10.0, 'reg_alpha': 0.5, 'eta': 0.05},\n",
    "    {'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.0, 'reg_lambda': 5.0, 'reg_alpha': 0.5, 'eta': 0.03},\n",
    "    {'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 1.0, 'reg_lambda': 10.0, 'reg_alpha': 1.0, 'eta': 0.03},\n",
    "    {'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.25, 'reg_lambda': 3.0, 'reg_alpha': 0.25, 'eta': 0.05},\n",
    "    {'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.5, 'reg_lambda': 5.0, 'reg_alpha': 0.5, 'eta': 0.05},\n",
    "]\n",
    "\n",
    "print('Grid size:', len(param_grid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ceb07",
   "metadata": {},
   "source": [
    "\n",
    "## Cross-validated search (5x StratifiedKFold)\n",
    "Early stopping uses each fold’s validation split only. We sort by PR-AUC, break ties with log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87fdeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1/8: {'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'eta': 0.05}\n",
      "Model 2/8: {'max_depth': 3, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 0.25, 'reg_lambda': 3.0, 'reg_alpha': 0.0, 'eta': 0.05}\n",
      "Model 3/8: {'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.85, 'colsample_bytree': 1.0, 'gamma': 0.0, 'reg_lambda': 5.0, 'reg_alpha': 0.0, 'eta': 0.05}\n",
      "Model 4/8: {'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.85, 'colsample_bytree': 1.0, 'gamma': 0.5, 'reg_lambda': 10.0, 'reg_alpha': 0.5, 'eta': 0.05}\n",
      "Model 5/8: {'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.0, 'reg_lambda': 5.0, 'reg_alpha': 0.5, 'eta': 0.03}\n",
      "Model 6/8: {'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 1.0, 'reg_lambda': 10.0, 'reg_alpha': 1.0, 'eta': 0.03}\n",
      "Model 7/8: {'max_depth': 4, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.25, 'reg_lambda': 3.0, 'reg_alpha': 0.25, 'eta': 0.05}\n",
      "Model 8/8: {'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.5, 'reg_lambda': 5.0, 'reg_alpha': 0.5, 'eta': 0.05}\n",
      "CV summary (top rows):\n",
      "   max_depth  min_child_weight  subsample  colsample_bytree  gamma  \\\n",
      "0          3                 1       0.90               0.9    0.0   \n",
      "1          3                 5       0.90               0.9    0.5   \n",
      "2          5                 1       0.80               0.8    0.0   \n",
      "3          4                 5       0.85               1.0    0.5   \n",
      "4          4                 1       0.85               1.0    0.0   \n",
      "\n",
      "   reg_lambda  reg_alpha   eta  mean_pr_auc  mean_roc_auc  mean_brier  \\\n",
      "0         1.0        0.0  0.05     0.322149      0.753449    0.198614   \n",
      "1         5.0        0.5  0.05     0.321878      0.753505    0.198597   \n",
      "2         5.0        0.5  0.03     0.321864      0.753180    0.195682   \n",
      "3        10.0        0.5  0.05     0.321793      0.753280    0.197344   \n",
      "4         5.0        0.0  0.05     0.321569      0.753108    0.197573   \n",
      "\n",
      "   mean_log_loss  median_best_iter  \n",
      "0       0.580681               453  \n",
      "1       0.580597               432  \n",
      "2       0.573907               360  \n",
      "3       0.577784               253  \n",
      "4       0.578355               255  \n",
      "Selected params: {'max_depth': 3, 'min_child_weight': 1.0, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 0.0, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'eta': 0.05}\n",
      "n_estimators (median best_iter): 453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "summary_records = []\n",
    "per_model_folds = []\n",
    "\n",
    "for idx, params in enumerate(param_grid, 1):\n",
    "    fold_metrics = []\n",
    "    fold_best_iters = []\n",
    "    print(f\"Model {idx}/{len(param_grid)}: {params}\")\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), 1):\n",
    "        X_tr, X_va = X_trainval.iloc[tr_idx], X_trainval.iloc[va_idx]\n",
    "        y_tr, y_va = y_trainval.iloc[tr_idx], y_trainval.iloc[va_idx]\n",
    "\n",
    "        neg, pos = (y_tr == 0).sum(), (y_tr == 1).sum()\n",
    "        spw = neg / pos\n",
    "\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=2000,\n",
    "            eval_metric='aucpr',\n",
    "            early_stopping_rounds=50,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            tree_method='hist',\n",
    "            max_delta_step=1,\n",
    "            scale_pos_weight=spw,\n",
    "            **params,\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "        proba_va = model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "        mb = metric_bundle(y_va, proba_va)\n",
    "        mb['fold'] = fold\n",
    "        fold_metrics.append(mb)\n",
    "        fold_best_iters.append(model.best_iteration)\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "    record = {\n",
    "        **params,\n",
    "        'mean_pr_auc': fold_df['pr_auc'].mean(),\n",
    "        'mean_roc_auc': fold_df['roc_auc'].mean(),\n",
    "        'mean_brier': fold_df['brier'].mean(),\n",
    "        'mean_log_loss': fold_df['log_loss'].mean(),\n",
    "        'median_best_iter': int(np.median(fold_best_iters)),\n",
    "    }\n",
    "    summary_records.append(record)\n",
    "    per_model_folds.append({'params': params, 'fold_metrics': fold_df})\n",
    "\n",
    "cv_summary_df = pd.DataFrame(summary_records)\n",
    "cv_summary_df = cv_summary_df.sort_values(['mean_pr_auc', 'mean_log_loss'], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print('CV summary (top rows):')\n",
    "print(cv_summary_df.head())\n",
    "\n",
    "best_params_raw = cv_summary_df.iloc[0][list(param_grid[0].keys())].to_dict()\n",
    "int_fields = {'max_depth'}\n",
    "best_params = {k: int(v) if k in int_fields else v for k, v in best_params_raw.items()}\n",
    "best_n_estimators = int(cv_summary_df.iloc[0]['median_best_iter'])\n",
    "print('Selected params:', best_params)\n",
    "print('n_estimators (median best_iter):', best_n_estimators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cecc40",
   "metadata": {},
   "source": [
    "\n",
    "## Fit final model on Train+Val (no test exposure)\n",
    "We keep the selected hyperparameters and the chosen number of boosting rounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baacc980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics (raw probabilities): {'roc_auc': 0.7589060007225699, 'pr_auc': 0.3323950699733614, 'brier': 0.1990220707227165, 'log_loss': 0.5817855265218022}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neg, pos = (y_trainval == 0).sum(), (y_trainval == 1).sum()\n",
    "spw_trainval = neg / pos\n",
    "\n",
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    eval_metric='aucpr',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    max_delta_step=1,\n",
    "    scale_pos_weight=spw_trainval,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "final_model.fit(X_trainval, y_trainval, eval_set=[(X_trainval, y_trainval)], verbose=False)\n",
    "\n",
    "y_test_proba_raw = final_model.predict_proba(X_test)[:, 1]\n",
    "raw_metrics = metric_bundle(y_test, y_test_proba_raw)\n",
    "print('Test metrics (raw probabilities):', raw_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac926ee8",
   "metadata": {},
   "source": [
    "\n",
    "## Probability calibration (isotonic, 3-fold on Train+Val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39b702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics (calibrated): {'roc_auc': 0.7594580133710067, 'pr_auc': 0.3327586072723112, 'brier': 0.09053498159354284, 'log_loss': 0.3105147948787612}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_for_cal = xgb.XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    max_delta_step=1,\n",
    "    scale_pos_weight=spw_trainval,\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "calibrated_model = CalibratedClassifierCV(base_for_cal, method='isotonic', cv=3)\n",
    "calibrated_model.fit(X_trainval, y_trainval)\n",
    "\n",
    "y_test_proba_cal = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "cal_metrics = metric_bundle(y_test, y_test_proba_cal)\n",
    "print('Test metrics (calibrated):', cal_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc9acf",
   "metadata": {},
   "source": [
    "\n",
    "## Confidence intervals via bootstrap on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2dd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap CI (raw) ROC-AUC: [0.75209598 0.75888742 0.76596995]\n",
      "Bootstrap CI (raw) PR-AUC : [0.32101209 0.33235081 0.34561623]\n",
      "Bootstrap CI (cal) ROC-AUC: [0.75277933 0.75944907 0.76663827]\n",
      "Bootstrap CI (cal) PR-AUC : [0.32158535 0.33262519 0.3459952 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Bootstrap CI (raw) ROC-AUC:', bootstrap_ci(roc_auc_score, y_test, y_test_proba_raw))\n",
    "print('Bootstrap CI (raw) PR-AUC :', bootstrap_ci(average_precision_score, y_test, y_test_proba_raw))\n",
    "print('Bootstrap CI (cal) ROC-AUC:', bootstrap_ci(roc_auc_score, y_test, y_test_proba_cal))\n",
    "print('Bootstrap CI (cal) PR-AUC :', bootstrap_ci(average_precision_score, y_test, y_test_proba_cal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec4ee6",
   "metadata": {},
   "source": [
    "\n",
    "## Threshold analysis (cost + operating points)\n",
    "- Grid thresholds 0.05–0.95.\n",
    "- Cost weights: FN=10, FP=1 (adjustable).\n",
    "- Scenarios: max accuracy, max F1, max balanced accuracy, min cost, recall>=0.70 if available.\n",
    "- Top-k capture/lift at 5%, 10%, 20% of applicants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcab4391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold grid (head):\n",
      "   threshold  accuracy  precision    recall        f1  balanced_accuracy  \\\n",
      "0       0.05  0.392500   0.151550  0.920081  0.260235           0.621630   \n",
      "1       0.10  0.635442   0.207228  0.757039  0.325386           0.688252   \n",
      "2       0.15  0.765028   0.266379  0.583375  0.365751           0.686136   \n",
      "3       0.20  0.826317   0.322845  0.451526  0.376494           0.663544   \n",
      "4       0.25  0.856472   0.368983  0.332153  0.349601           0.628758   \n",
      "\n",
      "      tn     fp    fn    tp     cost      lift  \n",
      "0  14588  30551   474  5457  35291.0  1.304947  \n",
      "1  27962  17177  1441  4490  31587.0  1.784372  \n",
      "2  35610   9529  2471  3460  34239.0  2.293709  \n",
      "3  39522   5617  3253  2678  38147.0  2.779919  \n",
      "4  41770   3369  3961  1970  42979.0  3.177198  \n",
      "Key scenarios:\n",
      "   threshold  accuracy  precision    recall        f1  balanced_accuracy  \\\n",
      "0       0.45  0.886822   0.578077  0.094251  0.162076           0.542606   \n",
      "1       0.20  0.826317   0.322845  0.451526  0.376494           0.663544   \n",
      "2       0.10  0.635442   0.207228  0.757039  0.325386           0.688252   \n",
      "3       0.10  0.635442   0.207228  0.757039  0.325386           0.688252   \n",
      "4       0.10  0.635442   0.207228  0.757039  0.325386           0.688252   \n",
      "\n",
      "        tn       fp      fn      tp     cost      lift  \\\n",
      "0  44731.0    408.0  5372.0   559.0  54128.0  4.977638   \n",
      "1  39522.0   5617.0  3253.0  2678.0  38147.0  2.779919   \n",
      "2  27962.0  17177.0  1441.0  4490.0  31587.0  1.784372   \n",
      "3  27962.0  17177.0  1441.0  4490.0  31587.0  1.784372   \n",
      "4  27962.0  17177.0  1441.0  4490.0  31587.0  1.784372   \n",
      "\n",
      "                    scenario  \n",
      "0               max_accuracy  \n",
      "1                     max_f1  \n",
      "2      max_balanced_accuracy  \n",
      "3          min_cost_FN10_FP1  \n",
      "4  recall>=0.70_max_accuracy  \n",
      "Top-k capture/lift:\n",
      "    scenario  volume_pct  precision    recall      lift     tn    fp    fn  \\\n",
      "0   top_5pct        0.05   0.461026  0.198449  3.969754  43763  1376  4754   \n",
      "1  top_10pct        0.10   0.373801  0.321868  3.218682  41941  3198  4022   \n",
      "2  top_20pct        0.20   0.295085  0.508177  2.540887  37939  7200  2917   \n",
      "\n",
      "     tp  \n",
      "0  1177  \n",
      "1  1909  \n",
      "2  3014  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "C_FN, C_FP = 10.0, 1.0\n",
    "thr_df = threshold_table(y_test, y_test_proba_cal, thresholds=np.linspace(0.05, 0.95, 19), cost_fn=C_FN, cost_fp=C_FP)\n",
    "scenarios_df = pick_scenarios(thr_df, C_FN, C_FP)\n",
    "\n",
    "print('Threshold grid (head):')\n",
    "print(thr_df.head())\n",
    "print('Key scenarios:')\n",
    "print(scenarios_df)\n",
    "\n",
    "topk_df = topk_capture(y_test, y_test_proba_cal, fracs=[0.05, 0.10, 0.20])\n",
    "print('Top-k capture/lift:')\n",
    "print(topk_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a91b12",
   "metadata": {},
   "source": [
    "\n",
    "## Save research artifact\n",
    "Includes model, calibrated model, feature names, CV summary, test metrics, threshold tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e896c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact: xgboost_loan_default_research_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "artifact = {\n",
    "    'best_params': best_params,\n",
    "    'best_n_estimators': best_n_estimators,\n",
    "    'cv_summary': cv_summary_df.to_dict(orient='records'),\n",
    "    'test_metrics_raw': raw_metrics,\n",
    "    'test_metrics_calibrated': cal_metrics,\n",
    "    'threshold_grid': thr_df.to_dict(orient='records'),\n",
    "    'threshold_scenarios': scenarios_df.to_dict(orient='records'),\n",
    "    'topk': topk_df.to_dict(orient='records'),\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'cost_weights': {'C_FN': C_FN, 'C_FP': C_FP},\n",
    "}\n",
    "\n",
    "with open('xgboost_loan_default_research_v2.pkl', 'wb') as f:\n",
    "    import pickle\n",
    "    pickle.dump(artifact, f)\n",
    "\n",
    "print('Saved artifact: xgboost_loan_default_research_v2.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1b278",
   "metadata": {},
   "source": [
    "\n",
    "## Notes / next experiments\n",
    "- Swap in time-based splits or OOT window if the data is temporal.\n",
    "- Add LightGBM and calibrated logistic regression baselines for comparative lift.\n",
    "- Produce SHAP/global importance plots for governance and monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
